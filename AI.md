Complete AI Notes and Revision

# Unit 1

## What is AI

### Foundations

#### Philosophy (the study of the fundamental nature of knowledge):

- Can formal rules be used to draw valid conclusions?
- How does the mind arise from a physical brain?
- Where does knowledge come from?
- How does knowledge lead to action?
- Aristotle (384–322 B.C.), was the first to formulate a precise set of laws governing the rational part of the mind. He developed an informal system of syllogisms for proper reasoning, which in principle allowed one to generate conclusions mechanically, given initial premises.
- Thomas Hobbes **(1588–1679)** proposed that reasoning was like numerical computation that we add and subtract in our silent thoughts.

#### Mathematics

- What are the formal rules to draw valid conclusions?
- What can be computed?
- Formal science required a level of mathematical formalization in three fundamental  areas:
  1. logic
  2. computation
  3. and probability.
  
1. Logic:
   - George Boole (1815–1864), who worked out the details of propositional, or Boolean logic.
   - In 1879, Gottlob Frege (1848–1925) extended Boole’s logic to include objects and relations, creating the firstorder logic that is used today.
   - First order logic contains predicates, quantifiers and variables E.g. 
  
  `Philosopher(a) ⇒ Scholar(a)`, `∀x, effect_corona(x) ⇒ quarantine(x)` , `∀x, King(x) ^ Greedy (x) ⇒ Evil (x)`

#### Economics
- How should we make decisions so as to maximize payoff?
- How should we do this when the payoff may be far in the future?

> [!info] The science of economics got its start in 1776, when Scottish philosopher ***Adam Smith*** treat it as a science, using the idea that economies can be thought of as consisting of individual agents maximizing their own economic well being.


#### Neuroscience (1861-present)

> [!faq] How do brains process information?

> [!info] Neuroscience is the study of the nervous system, particularly the brain.

- The exact way in which the brain enables thought is one of the great mysteries of science.
- It has been appreciated for thousands of years that the brain is somehow involved in thought, because of the evidence that strong blows to the head can lead to mental incapacitation.
- Brain is recognized as the seat of consciousness. Before then, candidate locations included the heart, the spleen, and the pineal gland. The truly amazing conclusion is that a collection of simple cells can lead to thought, action, and consciousness or, brains cause minds.
- It has also long been known that human brains are somehow different; in about 335 B.C. Aristotle wrote
> [!quote] Of all the animals, man has the largest brain in proportion to his size


#### PSYCHOLOGY

> [!faq] How do humans and animals think and act?

 
- The principle characteristic of cognitive psychology is that the brain processes and processes information. 
- The claim is that beliefs, goals, and reasoning steps can be useful components of a theory of human behavior. The knowledge-based agent has two key steps:
    1. Stimulus is translated into an internal representation
    2. The representation is manipulated by cognitive processes to derive new internal representations These are translated into actions 

#### COMPUTER ENGINEERING  

> [!faq] How can we build an efficient computer?
 
- For artificial intelligence to succeed, we need **two** things **intelligence and an artifact.**
- The computer has been the artifact of choice. The modern digital electronic computer was invented independently and almost simultaneously by scientists in three countries embattled in World War II.

### History

#### Maturation of Artificial Intelligence (1943-1952) 
1. **Year 1943**: The first work which is now recognized as AI was done by **Warren McCulloch** and **Walter Pits** in 1943. They proposed a model of **artificial neurons**. 
2. **Year 1949**: **Donald Hebb** demonstrated an updating rule for modifying the connection strength between neurons. His rule is now called **Hebbian learning**.
3. **Year 1950**: **Alan Turing** who was an English mathematician and pioneered Machine learning in 1950. Alan Turing published ***Computing Machinery and Intelligence*** in which he proposed a test. The test can check the machine's ability to exhibit intelligent behavior equivalent to human intelligence, called a **Turing test**. 

#### The birth of Artificial Intelligence (1952-1956)
1. **Year 1955**: *Allen Newell* and *Herbert A. Simon* created the first artificial intelligence program "Which was named as *Logic Theorist*. This program has proved 38 of 52 Mathematics theorems, and found new and more elegant proofs for some theorems. 
2. **Year 1956**: The word "Artificial Intelligence" first adopted by American Computer scientist **John McCarthy** at the Dartmouth Conference. For the first time, AI was coined as an academic field. At that time high-level computer languages such as FORTRAN, LISP, or COBOL were invented. And the enthusiasm for AI was very high at that time. The golden years-Early enthusiasm (1956-1974) 
3. **Year 1966**: The researchers emphasized developing algorithms which can solve mathematical problems. *Joseph Weizenbaum* created the first chatbot in 1966, which was named **ELIZA**. 
4. **Year 1972**: The first intelligent humanoid robot was built in Japan which was named as **WABOT1**

#### The first AI winter (1974-1980) 

- The duration between **1974 to 1980** was the first AI winter duration. 
- AI winter refers to the time period where computer scientists dealt with a severe shortage of funding from the government for AI research. 
- During AI winters, an interest of publicity on artificial intelligence was decreased. 

#### A boom of AI (1980-1987) 
- **Year 1980:** After AI winter duration, AI came back with **Expert System**. Expert systems were programmed that emulate the decision-making ability of a human expert. 
- In 1980, the first national conference of the American Association of Artificial Intelligence was held at Stanford University. 

#### The second AI winter (1987-1993)

- The duration between the years **1987 to 1993** was the second AI Winter duration. 
- Again Investors and government stopped funding for AI research due to high cost but not efficient result. The expert system such as XCON was very cost effective.

#### The emergence of intelligent agents (1993-2011) 

- **Year 1997:** In the year 1997, **IBM Deep Blue** beat world chess champion Gary Kasparov, and became the first computer to beat a world chess champion. 
- **Year 2002:** For the first time, AI entered the home in the form of Roomba, a vacuum cleaner. 
- **Year 2006:** AI came in the Business world till the year 2006. Companies like Facebook, Twitter, and Netflix also started using AI.

#### Deep learning, big data and artificial general intelligence (2011-present) 

- **Year 2011:** In the year 2011, **IBM's Watson** won jeopardy, a quiz show, where it had to solve complex questions as well as riddles. Watson had proved that it could understand natural language and can solve tricky questions quickly. 
- **Year 2012:** Google has launched an Android app feature **Google now**, which was able to provide information to the user as a prediction.
- **Year 2014:** In the year 2014, Chatbot **Eugene Goostman** won a competition in the infamous **Turing test**.
- **Year 2018:** The **Project Debater** from IBM debated on complex topics with two master debaters and also performed extremely well. 
- Google has demonstrated an AI program **Duplex** which was a virtual assistant and which had taken hairdresser appointments on call, and the lady on the other side didn't notice that she was talking with the machine.

### State of the Art


> [!faq] What can Al do today?
> A concise answer is difficult because there are so many activities in so many subfields. following is a few applications which shows the state of the art
 

1. **Speech recognition:** 
   A traveler calling United Airlines to book a flight can have the entire conversation guided by an automated speech recognition and dialog management system.
2. **Robotic Vehicles:**
   Today's robots are far more sophisticated than their predecessors. Many are semi-autonomous, with machine vision systems to interact within a changing environment. Some can even work side-by-side with humans. All signs suggest we are in the middle of a new industrial robot boom.
3. **Autonomous planning and scheduling:**
   A hundred million miles from Earth, NASA's Remote Agent program became the first on-board autonomous planning program to control the scheduling of operations for a spacecraft.
4. **AI Game Agents**:
   It is the design of artificial intelligence programs to be able to play more than one game successfully. For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For example, a chess-playing computer program cannot play checkers.
5. **Spam Fighting:**
   For starters, the spam filter now uses an artificial neural network to detect and block the especially sneaky or unhonest spam mails.
6. **Machine Translation:**
   A computer program automatically translates from Arabic to English, allowing an English speaker. Above are just a few examples of artificial intelligence systems that exist today. Not magic or science fiction-but rather science,engineering, and mathematics.
 

## Intelligent Agents

### What is an Agent? 


> [!info] An agent can be anything that perceive its environment through sensors and act upon that environment through actuators. 


- An Agent runs in the cycle of perceiving, thinking, and acting. 
- An agent gets percepts one at a time and maps this percept sequence to action. 
- Percepts are the electrical signals from sensors after processing objects in the visual field (like location, colors, loudness, direction etc.)

- An agent can be: 
- **Human-Agent:**
  A human agent has eyes, ears, and other organs which work for sensors and hand, legs, vocal tract work for actuators. 
- **Robotic Agent:**
  A robotic agent can have cameras, infrared range finder, NLP for sensors and various motors for actuators.
- **Software Agent:**
  Software agent can have keystrokes, file contents as sensory input and act on those inputs and display output on the screen. 

### Sensors, Actuators and Effectors

1. **Sensor:**
   Sensor is a device which detects the change in the environment and sends the information to other electronic devices. An agent observes its environment through sensors. 
2. **Actuators:**
   Actuators are the component of machines that converts energy into motion. The actuators are only responsible for moving and controlling a system.Actuator is a device that causes machines or  other devices to operate. An actuator can be an electric motor, gears, rails, etc. 
3. **Effectors:**
   Effectors are the devices which affect the environment. Effectors can be legs, wheels, arms, fingers, wings, fins, and display screen.

### Intelligent Agents 

> [!info] An intelligent agent is an autonomous entity which acts upon an environment using sensors and actuators for achieving goals. 

- An intelligent agent may learn from the environment to achieve their goals. Following are the main four rules for an AI agent
- **Rule 1:** An AI agent must have the ability to perceive the environment. 
- **Rule 2:** The observation must be used to make decisions.
- **Rule 3:** Decision should result in an action.
- **Rule 4:** The action taken by an AI agent must be a rational action. 

#### Structure of intelligent agent

> [!info] An intelligent agent is anything which perceives its environment, takes actions autonomously in order to achieve goals, & may improve its performance with learning or may use knowledge. 

- It interacts with surroundings via **perception** through sensors & **actions** through effectors or actuators.
$$Agent = Architecture + Agent\ Function + Agent\ Program $$
- The IA structure consists of three main parts: architecture, agent function, and agent program. 
- **Architecture:** This refers to machinery or devices that consist of actuators and sensors. The intelligent agent executes on this machinery. Examples include a personal computer, a car, or a camera.
- **Agent function:** This is a function in which actions are mapped from a certain percept sequence. $f: P* → A$
    - Percept sequence refers to a history of what the intelligent agent has perceived.
- **Agent program:** This is an implementation or execution of the agent function. The agent function is produced through the agent program’s execution on the physical architecture.

#### Characteristics of intelligent agents

Intelligent agents have the following distinguishing characteristics:

1. They have some level of autonomy that allows them to perform certain tasks on their own.
2. They have a learning ability that enables them to learn even as tasks are carried out.
3. They can interact with other entities such as agents, humans, and systems.
4. New rules can be accommodated by intelligent agents incrementally.
5. They exhibit goal-oriented habits.
6. They are knowledge-based. They use knowledge regarding communications, processes, and entities.

![[working_of_agent.jpg]]


### Interaction of Agents with Environment

- Interaction of the Agent with the environment uses **sensors and effectors**. Sensors perceive the environment and the actuators or effectors act upon that environment.
- This interaction can occur in two different ways:
- **Perception:** Perception is a passive interaction between the agent and the environment where the environment remains unchanged when the agent takes up information from the environment. This involves gaining information using **sensors** from the surroundings without any change to the surroundings.
- **Action:** Action is an **active** interaction between the agent and the environment where the environment changes when the action is performed. This involves utilization of an **effector** or an **actuator** which completes an action but leads to changes in the surroundings while doing so.
- For example, in the case of a virtual agent, when the virtual agent reads and interprets the information provided by the user, it is known as **perception** while when it replies to the user based on the interpretation it is known as **action**.

### Rational Agents


> [!info] A rational agent is an agent which has clear preference, models uncertainty, and acts in a way to maximize its performance measure with all possible actions.

 - A rational agent is said to perform the right things. 
 - AI is about creating rational agents to use for game theory and decision theory for various real-world scenarios. 
 - For an AI agent, the rational action is most important because in the AI reinforcement learning algorithm, for each best possible action, the agent gets the positive reward and for each wrong action, an agent gets a negative reward. 

#### Rationality

The rationality of an agent is measured by its performance measure. Rationality can be judged on the basis of following points: 

1. Performance measure which defines the success criterion.
2. Agent prior knowledge of its environment. 
3. Best possible actions that an agent can perform.
4. The sequence of percepts.

### PEAS Representation

- PEAS is a type of model on which an AI agent works upon. 
- When we define an AI agent or rational agent, then we can group its properties under the PEAS representation model.  It is made up of four words: 

- **P:** Performance measure 
- **E:** Environment 
- **A:** Actuators 
- **S:** Sensors 

1.Fully observable vs. partially observable. 
➢ If an agent's sensors give it access to the complete state of the environment  at  each point in time, then we say that the task environment  is fully observable. 
➢ An environment   might   be partially observable   because   of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data.

### Fully observable vs. partially observable

- If an agent's sensors give it access to the complete state of the environment  at  each point in time, then we say that the task environment  is fully observable. 
- An environment might be partially observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data.

### Deterministic vs. Stochastic.

- If the next state of the environment is completely determined by the current state and the action executed by the agent, then we say the environment is deterministic; 
- Otherwise, it is stochastic.

### Episodic vs. sequential 

- In an **episodic task environment**, the agent's experience is divided into atomic episodes. 
- Each episode consists of the agent perceiving and then performing a single action. 
- Crucially, the next episode does not depend on the actions taken in previous episodes. 
- For example, an agent that has to spot defective parts on an assembly line bases each decision on the current part, regardless of previous decisions; 
- In **sequential environments**, on the other hand, the current decision Could affect all future decisions.
- Chess and taxi driving are sequential.

### Discrete vs. continuous
- The discrete/continuous distinction can be applied  to the state of the environment, to the way time is handled, and to the percepts and actions of the agent. 
- For example, a **discrete state** environment such as a chess game has a finite number of distinct states.
- Chess also has a discrete set of percepts and actions. 
- Taxi driving is a **continuous state** and **continuous time**  problem. 
- The speed and location of the taxi and of the other vehicles sweep through a range of continuous values and do so smoothly over time. 
- Taxi-driving actions are also continuous (steering angles, etc.)

### Single agent vs. multiagent

- An agent solving a crossword puzzle by itself is clearly in a **single agent** environment.
- Where as an agent playing chess is in a **two agent** environment. 
- Multiagent  is further classified in to two ways
   - *Competitive* multiagent environment 
   - *Cooperative* multiagent environment

### Types of Agents in Artificial Intelligence With Examples

Based on their degree of perceived intelligence and capability, types of agents in artificial intelligence can be divided into:
1. [[#Simple Reflex Agents]]
2. [[#Model Based Agents]]
3. [[#Goal Based Agents]]
4. [[#Utility Based Agents]]
5. [[#Learning Agents]]
Performance can be improved and better action can be generated for each of these types of agents in AI.

#### Simple Reflex Agents

> [!info] These agents take decisions on the basis of the current percepts and ignore the rest of the percept history. 

- The agent function, in this case, is based on **condition-action** rule where the condition or the state is mapped to the action such that action is taken only when condition is true or else it will not.
- If the environment associated with this agent is **fully observable**, only then the agent function is successful, if it is partially observable, in that case the agent function enters into *infinite loops* that can be escaped only on randomization of its actions.
- The problems associated with this type include very **limited intelligence**, No knowledge of non-perceptual parts of the state, huge size for generation and storage and inability to adapt to changes in the environment.
- **Example:** A thermostat in a heating system.

#### Model Based Agents

> [!info] Model-based agent utilizes the condition-action rule, where it works by finding a rule that will allow the condition, which is based on the current situation, to be satisfied. 

- Irrespective of the first type, it can handle **partially observable environments** by tracking the situation and using a particular *model related* to the world.
- It consists of two important factors, which are **Model and Internal State**.
- Model provides knowledge and understanding of the process of occurrence of different things in the surroundings such that the current situation can be studied and a condition can be created. Actions are performed by the agent based on this model.
- Internal State uses the **perceptual history** to represent a current percept. The agent **keeps a track** of this internal state and is adjusted by each of the percepts.
- The current internal state is stored by the agent inside it to maintain a kind of structure that can describe the unseen world.
- The state of the agent can be updated by gaining information about how the world evolves and how the agent's action affects the world.
- **Example:** A vacuum cleaner that uses sensors to detect dirt and obstacles and moves and cleans based on a model.

#### Goal Based Agents

> [!info] This type takes decisions on the basis of its goal or desirable situations so that it can choose such an action that can achieve the goal required. 

- It is an improvement over model based agent where **information about the goal is also included**. This is because it is not always sufficient to know just about the current state, **knowledge of the goal is a more beneficial** approach.
- The aim is to **reduce the distance between action and the goal** so that the best possible way can be chosen from multiple possibilities.
- Once the best way is found, the decision is represented explicitly which makes the agent more flexible.
- It carries out **considerations of different situations** called **searching and planning** by considering long sequence of possible actions for confirming its ability to achieve the goal. This makes the agent proactive.
- It can easily change its behavior if required.
- **Example:** A chess playing AI whose goal is winning the game.

#### Utility Based Agents


> [!NOTE]  A utility-based agent is an agent that acts based not only on what the goal is, but the best way to reach that goal.

- There are **multiple sequences in which a goal state can be achieved**. Some sequences **may be better than** the others.
- The **success or failure** of this agent depends on the **choice of the appropriate route** taken to reach to the goal.
- The difference in Goal Based and Utility Based Agents lies in the fact that Goal Based Agents only measure if a state can be a possible goal state or not.
- But the Utility Based Agents keep a measure of the desirability (preference / utility) of these multiple routes to reach the goal state. This measure is kept by a utility function. The utility function maps a state with a real number which describes the happiness degree of the agent.
- A Utility Based Agent has to keep a regular track of its environment. This enables it to reach a goal state in the quickest, cheapest, and safest way. A rational agent selects the best possible action based on the utility of the output. Hence these agents involve a great deal of perception, reasoning, learning and representation.

#### Learning Agents

> [!info] As the name suggests, a learning agent has the capability to learn from its past experiences. 

- It starts its working in **unknown environments** by using its **basic knowledge** and then it automatically **adapts to the environment** through its **learning process** to reach its goal. 

- These agents comprise of the following components
- **Learning Element**
  It is through the learning element that the agent learns about the changes in the environment.
- **The Critic**
  The critic gives feedback to the learning agent. The critic evaluates the feedback based on the performance standards fixed in advance. The agent comes to know about its performance based on the feedback given to it by the critic.
- **Performance Element**
  This is responsible for selecting the best external action for the agent.
- **Problem Generator**
  This component suggests actions for the agent which are based on new informative experiences. It also keeps track of the past.

These agents are hence capable of **learning, analyzing their performance**, and finding **new and innovative** ways to improve their performance.

## Chapter 3

### BFS

> [!info] Breadth-First Search (BFS) is a graph traversal algorithm that explores nodes layer by layer, starting from a given source node. It uses a queue data structure to keep track of nodes to visit next and ensures that nodes closer to the source are visited before farther ones. 

#### Steps of BFS

1. **Initialize:** Start from a source node and mark it as visited. Add it to a queue.
2. **Explore:** **Dequeue** the front node, examine all its **unvisited adjacent nodes**, mark them as visited, and enqueue them.
3. **Repeat:** Continue **dequeuing** nodes and exploring their neighbors until the queue is empty.
4. **Terminate:** When the queue is empty, all reachable nodes have been visited.

#### Advantages:

- **Finds shortest path** in an unweighted graph (in terms of number of edges).
- **Completeness:** Guaranteed to visit all reachable nodes from the source.
- **Layer-wise exploration** allows solving problems like finding the shortest path in unweighted grids or graphs.

#### Disadvantages:

- **Memory-intensive:** It stores all the nodes at the current level, which can be large, especially for wide graphs.
- *Slower than DFS* for deep graphs, as it explores breadth first, potentially visiting many nodes before finding a target.

BFS is especially useful in scenarios like maze solving, shortest path discovery, and level-order traversal in trees.


### DFS

> [!info] Depth-First Search (DFS) is a graph traversal algorithm that explores as far as possible along a branch before backtracking. It uses a stack (often implemented using recursion) to keep track of nodes to visit. 

#### Steps of DFS

1. **Initialize:** Start from a source node, mark it as visited.
2. **Explore:** Visit an adjacent unvisited node, mark it, and continue down that path.
3. **Backtrack:** If no unvisited neighbors remain, backtrack to the previous node and explore its other neighbors.
4. **Repeat:** Continue this process until all nodes are visited or the target node is found.
5. **Terminate:** When all possible paths have been explored, the algorithm stops.

#### Advantages

- **Memory-efficient:** DFS only needs to store the current path, so memory usage is lower than BFS in wide graphs.
- **Useful for deep graphs:** It can reach deep parts of the graph quickly, making it good for problems like solving mazes.
- **Easier implementation with recursion** and useful in exploring all possible solutions (e.g., puzzles, decision trees).

#### Disadvantages

- **Not guaranteed to find the shortest path** (especially in unweighted graphs), since it dives deep into paths before exploring others.
- **Can get stuck in deep, infinite paths** if the graph is very deep or contains cycles (unless handled with cycle detection or depth limits).

DFS is often used in topological sorting, connected components, and pathfinding in specific use cases where depth matters more than the shortest path.

### DLS

> [!info] Depth-Limited Search (DLS) is a variant of the Depth-First Search (DFS) algorithm that adds a limit on the depth of exploration. It is useful in scenarios where there is a risk of going too deep into a graph or tree, which can lead to excessive memory usage or infinite loops. 



###### Steps of DLS


1. **Initialize:** Start from a source node, mark it as visited, and set a depth limit.
2. **Explore:**
    - If the current **depth is less than the limit**, visit an adjacent unvisited node, mark it, and **recursively perform DLS** on that node, increasing the depth by one.
    - If the current depth reaches the limit, backtrack.
3. **Backtrack:** If no unvisited neighbors remain or if the depth limit is reached, backtrack to the previous node and explore its other neighbors.
4. **Repeat:** Continue this process until all possible paths up to the depth limit have been explored or until the target node is found.
5. **Terminate:** The search stops when all paths within the depth limit are explored.

###### Advantages

- **Controlled Depth:** Helps prevent going too deep into infinite paths, which is useful for large or infinite graphs.
- **Memory-efficient:** Since it uses depth-first traversal, it generally requires less memory than breadth-first search.
- **Good for certain problem domains:** DLS can be effective in situations where solutions are expected to be found at a certain depth.

###### Disadvantages:

- **Not complete:** If the solution is deeper than the depth limit, DLS may miss it entirely.
- **Not optimal:** Like DFS, DLS does not guarantee finding the shortest path in terms of the number of edges.
- **Requires a depth limit:** Determining an appropriate depth limit can be challenging and may require prior knowledge of the problem domain.

DLS is useful in scenarios such as puzzle-solving, artificial intelligence in games, or exploring decision trees where the depth of potential solutions is known or can be reasonably estimated.

### IDDFS

> [!info] Iterative Deepening Depth-First Search (IDDFS) is a hybrid algorithm that combines the depth-first search (DFS) approach with the breadth-first search (BFS) strategy. It effectively addresses the limitations of both algorithms, particularly in terms of memory usage and the ability to find the shortest path in unweighted graphs. 


***=Steps of IDDFS=***

1. **Initialize:** Start with a depth limit of 0.
2. **Depth-Limited Search:** Perform a depth-limited search (DLS) from the root node up to the **current depth limit**.
    - If the goal is found, return the path.
    - If the depth limit is reached **without finding** the goal, **increment the depth limit** by 1.
3. **Repeat:** Continue performing DLS with increasing depth limits until the goal node is found.
4. **Terminate:** The search stops when the goal is located, or all nodes up to the maximum depth limit are explored.

***=Advantages=***

- **Complete:** IDDFS is complete, meaning it will find a solution if one exists, similar to BFS.
- **Memory-efficient:** It uses the memory of DFS (O(bd), where b is the branching factor and d is the depth), as it only needs to store the current path.
- **Optimal for unweighted graphs:** Like BFS, it guarantees the shortest path in terms of the number of edges in unweighted graphs.
- **Iterative approach:** It gradually explores deeper levels without needing to store all nodes at a given level, making it suitable for large or infinite graphs.

***=Disadvantages:=***

- **Time-consuming:** IDDFS may revisit the same nodes multiple times as it incrementally increases the depth limit, leading to higher time complexity compared to other methods (O(b^d) in the worst case).
- **Depth limit management:** Choosing an appropriate depth limit can be challenging, and if the solution is significantly deeper, the algorithm may take longer to find it.

***=Use Cases=***

- The depth of the solution is unknown.
- The state space is large or infinite (e.g., puzzles, game trees).
- Memory is a constraint, and the graph is not necessarily well-structured.

### Uniform Cost Search

> [!info] Uniform Cost Search (UCS) is an informed search algorithm that is used to find the least-cost path from a start node to a goal node in a weighted graph. It is a variant of Dijkstra's algorithm and is particularly effective when dealing with graphs where the cost of moving from one node to another varies. 


- **Cost-Effective:** UCS expands the least costly node first, ensuring that it always finds the optimal (least-cost) solution if one exists.
- **Priority Queue:** The algorithm uses a priority queue (often implemented with a min-heap) to manage the nodes being explored based on their cumulative cost from the start node.
- **Exploration:** As it explores the graph, UCS keeps track of the cost to reach each node, adding nodes to the priority queue with their respective cumulative costs.

###### Steps of Uniform Cost Search:

1. **Initialize:** Start with the initial node, adding it to the priority queue with a cost of 0.
2. **Expand Nodes:** While the priority queue is not empty:
    - Dequeue the node with the lowest cumulative cost.
    - If this node is the goal, return the path and the total cost.
    - If it’s not the goal, enqueue all its unvisited neighbors with the updated cumulative costs.
3. **Terminate:** The search continues until the goal node is found or all possible nodes are explored (in which case, there is no path to the goal).

###### Advantages:

- **Optimality:** UCS guarantees finding the least-cost path if the edge costs are non-negative.
- **Completeness:** It will find a solution if one exists, provided the search space is finite.
- **Flexible Cost Handling:** It can handle varying costs associated with transitions between nodes.

###### Disadvantages:

- **Memory Intensive:** UCS requires memory proportional to the number of nodes it generates, which can be substantial for large graphs.
- **Time Complexity:** In the worst case, it can explore a large portion of the graph, leading to longer runtimes, particularly if there are many paths to evaluate.

### A* Algorithm Overview

> [!info] A is a heuristic search algorithm used for finding the least-cost path from a start node to a goal node in a weighted graph. It combines the benefits of uniform cost search and greedy best-first search. 

1. **Cost Function f(n)**: Total estimated cost of the cheapest path through node n: $$f(n)=g(n)+h(n)$$
2. **Actual Cost g(n)**: Cumulative cost to reach node $n$ from the start.
3. **Heuristic Cost h(n)**: Estimated cost from node $n$ to the goal (should be admissible, meaning it does not overestimate).

***=Steps of the A\* Algorithm=***

1. **Initialize**
    - Create an **Open List** (nodes to be evaluated) and a **Closed List** (evaluated nodes).
    - Set $g(start) = 0$ and calculate $h(start)$ and $f(start)$.
2. **Main Loop**:
    - While the Open List is not empty:
        1. Select the node n with the lowest $f(n)$.
        2. If $n$ is the goal, return the path.
        3. Move $n$ to the Closed List.
        4. For each neighbor $m$ of $n$:
            - If $m$ is in the Closed List, skip it.
            - Calculate g(m) and if $m$ is not in the Open List or this **g(m) is lower**, update g(m), h(m), and f(m). Add $m$ to the Open List if it's not already there.
3. **Termination**:

    - If the Open List is empty and the goal was not found, return failure.


### Greedy First Search 

> [!info] Greedy First Search is a heuristic search algorithm that aims to find the shortest path to a goal by making locally optimal choices at each step, with the hope of finding a global optimum. It uses a heuristic function to estimate the cost from the current node to the goal and selects the node that appears to be closest to the goal. 

- **Heuristic-Based:** Greedy First Search relies heavily on a **heuristic function** $h(n)$ that estimates the cost or distance from a node $n$ to the goal. This function **drives the search process**.
- **Fast and Efficient:** By focusing on the node that seems closest to the goal, the algorithm can often find a solution quickly.
###### Steps of Greedy First Search:

1. **Initialize:** Start with the initial node and add it to a priority queue (or open list) based on the heuristic value.
2. **Expand Nodes:**
    - While the priority queue is not empty:
        - **Dequeue** the node with the **lowest** heuristic value $h(n)$.
        - If this node is the **goal**, return the **path and the total cost**.
        - Otherwise, **generate all its neighbors** and calculate their **heuristic values**.
        - **Add** the **neighbors** to the **priority queue**.
3. **Terminate:** The search continues until the goal node is found or all possible nodes are explored.


### Informed Search

> [!info] Informed Search refers to a class of search algorithms that utilize additional information, typically in the form of heuristic functions, to guide the search process towards the goal more efficiently than uninformed search strategies.


- By incorporating **domain-specific knowledge**, informed search algorithms can reduce the number of nodes evaluated and speed up the search.
- **Heuristic Functions**: Informed search algorithms use **heuristic functions h(n)** to estimate the cost from a node n to the goal, allowing them to **prioritize nodes** that are more likely to lead to the goal quickly.
- **Efficiency**: These algorithms can significantly decrease the search space and improve performance, particularly in large or complex problem domains.
- **Optimality**: Some informed search algorithms (like A*) guarantee finding the optimal solution if the heuristic is admissible.
- **Heuristic Dependence**: The effectiveness of informed search relies heavily on the quality of the heuristic. A poor heuristic can lead to inefficient searches.

### Uninformed Search

> [!info] Uninformed Search refers to a class of search algorithms that operate without any additional information about the goal's location. These algorithms explore the search space based solely on the structure of the problem, making decisions based only on the current state and available actions.

- They do not utilize heuristics or domain-specific knowledge to guide the search. 
- **Blind Search**: Uninformed search strategies treat all paths equally, exploring them systematically without bias towards promising paths.
- **No Heuristic Function**: These algorithms do not use heuristic information to estimate costs or distances to the goal.
- **Simplicity**: Uninformed search algorithms are straightforward to implement and understand.
- **Inefficiency**: Uninformed search can be highly inefficient, particularly in large search spaces, as they do not prioritize promising paths.

### Difference between informed and uninformed search

Here's a comparison of **Informed Search** and **Uninformed Search** in a table format highlighting their key differences:

| **Aspect**             | **Informed Search**                                                   | **Uninformed Search**                                                                         |
| ---------------------- | --------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| **Definition**         | Utilizes heuristic information to guide the search.                   | Operates without any additional information about the goal.                                   |
| **Heuristic Function** | Uses heuristics \(h(n)\) to estimate costs to the goal.               | Does not use heuristics; relies solely on the structure of the search space.                  |
| **Efficiency**         | More efficient due to focused exploration of promising paths.         | Can be inefficient as it explores paths uniformly without bias.                               |
| **Examples**           | A*, Greedy Best-First Search                                          | Breadth-First Search (BFS), Depth-First Search (DFS), Uniform Cost Search                     |
| **Optimality**         | Can guarantee optimality if the heuristic is admissible (e.g., A*).   | May not guarantee optimal solutions (e.g., DFS can miss shorter paths).                       |
| **Memory Usage**       | Often requires more memory to store additional heuristic information. | Generally requires less memory than informed search, especially in DFS.                       |
| **Completeness**       | Usually complete, assuming a good heuristic is used.                  | Complete for algorithms like BFS but not for all (e.g., DFS can get stuck in infinite paths). |
| **Search Strategy**    | Prioritizes nodes based on estimated cost and heuristic guidance.     | Explores nodes systematically based on their order in the search process.                     |

### Heuristic Function

> [!info] A heuristic function is a key component in search algorithms, particularly in informed search strategies. It provides an estimate of the cost or distance from a given node to the goal node.

- The **primary purpose** of a heuristic function is to **guide the search** process, helping algorithms make more **informed decisions** about which paths to explore. 
- The heuristic function, often denoted as **h(n)**, evaluates the estimated cost to reach the goal from node $n$.
- **Domain-Specific**: Heuristics are typically designed based on knowledge of the specific problem domain, meaning they can vary widely in different contexts.
- **Admissibility**: A heuristic is considered **admissible** if it never overestimates the true cost to reach the goal. This property is crucial for ensuring the optimality of search algorithms like A*.

# Unit 2

## Decision tree

> [!info] A decision tree is a type of supervised machine learning used to categorize or make predictions based on how a previous set of questions were answered. The model is a form of supervised learning, meaning that the model is trained and tested on a set of data that contains the desired categorization.

### Some key terms of a decision tree

- **Root node:** The base of the decision tree.
- **Splitting:** The process of dividing a node into multiple sub-nodes.
- **Decision node:** When a sub-node is further split into additional sub-nodes.
- **Leaf node:** When a sub-node does not further split into additional sub-nodes; represents possible outcomes.
- **Pruning:** The process of removing sub-nodes of a decision tree.
- **Branch:** A subsection of the decision tree consisting of multiple nodes.

### Advantages of Decision Trees
- Works for numerical or categorical data and variables.
- Models problems with multiple outputs.
- Tests the reliability of the tree.
- Requires less data cleaning than other data modeling techniques.

### Disadvantages
- Affected by noise in the data.
- Not ideal for large datasets.
- Can disproportionately value, or weigh, attributes.
- The decisions at nodes are limited to binary outcomes, reducing the complexity that the tree can handle.
- Trees can become very complex when dealing with uncertainty and numerous linked outcomes.

***Factor for Decision Trees***
1. **Entropy:**
    - Entropy basically tells us how impure a collection of data is. The term impure here defines non-homogeneity.
    - In other word we can say, **Entropy** is the measurement of homogeneity. It returns us the information about an arbitrary dataset that how impure/non-homogeneous the data set is.
    - Entropy controls how a Decision Tree decides to split the data


## Learning from examples

An agent is learning if it improves its performance on future tasks after making observations about the world.

### Forms of learning

- Any component of an agent can be **improved** by **learning from data**. The improvements, and the techniques used to make them, depend on four major factors:
    - Which component is to be improved.
    - What Prior knowledge the agent already has.
    - What Representation is used for the data and the component. 
    - What Feedback is available to learn from.

There are three types of learning
#### Supervised Learning

- Supervised learning is an learning method in which a model learns from a **labeled dataset** containing **input-output** pairs. 
- Each input in the dataset has a **corresponding correct output** (the label), and the model's task is to learn the **relationship** between the **inputs and outputs**.
- This enables the model to make predictions on new, unseen data by applying the learned mapping.

***Example of Supervised Learning***

- **Predicting house prices:** The input might be house features such as size, location, and number of bedrooms, and the output would be the house price. The supervised learning model would learn the relationship between these features and house prices from historical data, and then it could predict prices for new houses entering the market.

***Categories of Supervised Learning***

- **Regression:** When dealing with real-valued output variables like "price" or "temperature," several popular Regression algorithms come into play, such as the Simple Linear Regression Algorithm, Multivariate Regression Algorithm, Decision Tree Algorithm, and Lasso Regression.

- **Classification:** In instances where the output variable is a category, like distinguishing between 'spam' and 'not spam' in email filtering, several widely-used classification algorithms come into play. These encompass the following algorithms: Random Forest, Decision Tree, Logistic Regression, and Support Vector Machine.

***Advantages of Supervised Learning***

- **Effectiveness:** Supervised learning can predict outcomes based on past data.
- **Simplicity:** It's relatively easy to understand and implement.
- **Performance Evaluation:** It is easy to measure the performance of a supervised learning model since the ground truth (labels) is known.
- **Applications:** Can be used in various fields like finance, healthcare, marketing, etc.
- **Feature Importance:** It allows an understanding of which features are most important in making predictions.

***=Disadvantages of Supervised Learning=***
- **Dependency on Labeled Data:** Supervised learning requires a large amount of labeled data, which can be expensive and time-consuming.
- **Overfitting:** Models can become too complex and fit the noise in the training data rather than the actual signal, which degrades their performance on new data.
**Generalization:** Sometimes, these models do not generalize well to unseen data if the data they were trained on does not represent the broader context.

***=Applications of Supervised Learning=***

- ***Healthcare:*** Used to predict patient diagnoses based on symptoms and past medical history.
- **Finance:** For credit scoring and predicting stock prices.
- **Retail:** To forecast sales, recommend products, and personalize marketing.
- **Autonomous Vehicles:** These are used to recognize traffic signs and pedestrians.
- **Speech Recognition:** In virtual assistants and transcription services. 

#### Unsupervised Learning

> [!info]
> In unsupervised learning the agent learns patterns in the input even though no explicit feedback is supplied.  

- Unlike supervised learning, where the training data includes both input vectors and corresponding target labels, unsupervised learning algorithms try to **learn patterns and relationships** directly from the **input data**.

***Example of Unsupervised Learning***

**Clustering:** 
- A common unsupervised learning technique is clustering, where data is grouped into subsets (clusters) such that data in each cluster are more similar than those in others.
- For instance, a company could use clustering to segment its customers based on purchasing behavior without prior knowledge of the customer groups' characteristics.

**Categories of Unsupervised Learning**

**Clustering:** Grouping similar instances into clusters (e.g., k-means, hierarchical clustering). Some popular clustering algorithms are the K-Means Clustering algorithm, Mean-shift algorithm, DBSCAN Algorithm, Principal Component Analysis, and Independent Component Analysis.
Association: Discovering rules that capture interesting relationships between variables in large databases (e.g., market basket analysis). Some popular algorithms of Association are the Apriori Algorithm, Eclat, and FP-growth algorithm.
Dimensionality Reduction: Reducing the number of random variables under consideration (e.g., PCA, t-SNE), which helps to simplify the data without losing important information.


**Advantages of Unsupervised Learning**

- Discovering Hidden Patterns: It can identify patterns and relationships in data that are not initially evident.
- No Need for labeled data,  works with unlabeled data, making it useful where obtaining labels is expensive or impractical.
- Reduction of Complexity in Data: Helps reduce the dimensionality of data, making complex data more comprehensible.
- Feature Discovery: This can be used to find useful features that can improve the performance of supervised learning algorithms.
- Flexibility: Can handle changes in input data or the environment since it doesn’t rely on predefined labels.

**Disadvantages of Unsupervised Learning**
- Interpretation of Results: The results can be ambiguous and harder to interpret than those from supervised learning models.
- Dependency on Input Data: The output quality heavily depends on the quality of the input data. Lack of Precise Objectives: Without specific tasks like prediction or classification, the direction of learning is less focused, leading to less actionable insights.

**Applications of Unsupervised Learning**

- **Customer Segmentation:** Businesses use clustering to segment customers based on behaviors and preferences for targeted marketing.
- **Anomaly Detection:** Identifying unusual data points can be critical in fraud detection or network security.
- **Recommendation Systems:** Associative models help build recommendation systems that suggest products based on user behavior.
- **Feature Elicitation:** Used in preprocessing steps to extract new features from raw data which can improve the accuracy of predictive models.
- **Image Segmentation:** Applied in computer vision to divide an image into meaningful segments and analyze each segment individually.


#### Reinforcement Learning

> [!info] In reinforcement learning the agent learns from a series of reinforcements - rewards or punishments. 

- For example, the lack of a tip at the end of the journey gives the taxi agent an indication that it did something wrong.

***Example of Reinforcement Learning***

- **Chess game:** A classic example of reinforcement learning is the game of chess. In this scenario, the RL agent learns to play chess by playing games against opponents.
- Each move the agent makes results in a new board state and possibly a reward (such as capturing an opponent's piece) or a penalty (such as losing a piece).
- The agent learns effective strategies over time by maximizing its cumulative rewards (ultimately aiming to win games).

***Categories of Reinforcement Learning***

- **Model-based RL:** In this category, the agent builds a model of the environment and uses it to predict future rewards and states. This allows the agent to plan by considering potential future situations before taking action.
- **Model-free RL:** Here, the agent learns to act without explicitly constructing a model of the environment. It directly learns the value of actions or action policies from experience.
- **Partially Observable RL:** This type involves situations where the agent doesn't have access to the full state of the environment. The agent must learn to make decisions based on incomplete information, often using strategies that involve maintaining internal state estimates.

**Advantages of Reinforcement Learning**

- **Adaptability:** RL agents can adapt to new environments or changes within their environment, making them suitable for dynamic and uncertain situations.
- **Decision-Making Autonomy:** RL agents make decisions based on learned experiences rather than pre-defined rules, which can be advantageous in complex environments where manual behavior specification is impractical.
- **Continuous Learning:** Since the learning process is continuous, RL agents can improve their performance over time as they gain more experience.
- **Handling Complexity:** RL can handle problems with high complexity and numerous possible states and actions, which might be infeasible for traditional algorithms.
- **Optimization:** RL is geared towards optimization of the decision-making process, aiming to find the best sequence of actions for any given situation.

***Disadvantages of Reinforcement Learning***

- **Dependency on Reward Design:** The effectiveness of an RL agent is heavily dependent on the design of the reward system. Poorly designed rewards can lead to unwanted behaviors.
- **High Computational Cost:** Training RL models often requires significant computational resources and time, especially as the complexity of the environment increases.
- **Sample Inefficiency:** RL algorithms typically require many interactions with the environment to learn effective policies, which can be impractical in real-world scenarios where each interaction could be costly or time-consuming.

***Applications of Reinforcement Learning*** ^491b57

- **Autonomous Vehicles:** RL is used to develop autonomous driving systems, helping vehicles learn to navigate complex traffic environments safely.
- **Robotics:** RL enables robots to learn complex tasks like walking, picking up and manipulating objects, and interacting with humans and other robots in a dynamic environment.
- **Gaming:** In the gaming industry, RL is used to develop AI that can challenge human players, adapt to their strategies, and provide engaging gameplay.
- **Finance:** RL can be applied to trading and investment strategies where the algorithm learns to make buying and selling decisions to maximize financial return.
- **Healthcare:** RL algorithms are being explored for various applications in healthcare, including personalized treatment recommendation systems and management of healthcare logistics.

#### Ensemble Learning

> [!info] Ensemble learning is a combination of several learning models in one problem. These models are known as weak learners.  

- The intuition is that when you combine several weak learners, they can become strong learners.
- Each weak learner is fitted on the training set and provides predictions obtained. 
- The final prediction result is computed by combining the results from all the weak learners.

***Basic ensemble learning techniques***

***Max voting***

- In classification, the prediction from each model is a vote.
- In max voting, the final prediction comes from the prediction with the most votes.
- Let’s take an example where you have three classifiers with the following predictions:
1. classifier 1 – class A
2. classifier 2 – class B
3. classifier 3 – class B

The final prediction here would be class B since it has the most votes. 

***Averaging***

- In averaging, the final output is an average of all predictions. 
- This goes for regression problems. For example, in random forest regression, the final result is the average of the predictions from individual decision trees. 

- Let’s take an example of three regression models that predict the price of a commodity as follows:

1. regressor 1 – 200
2. regressor 2 – 300 
3. regressor 3 – 400

The final prediction would be the average of 200, 300, and 400. 

***Weighted average***

- In weighted averaging, the base model with higher predictive power is more important. In the price prediction example, each of the regressors would be assigned a weight. 
- The sum of the weights would equal one.
- Let’s say that the regressors are given weights of 0.35, 0.45, and 0.2 respectively. The final model prediction can be computed as follows:
$$0.35 * 200 + 0.45*300 + 0.2*400 = 285$$


##### Bagging and Boosting

Bagging and Boosting are two types of Ensemble Learning. These two decrease the variance of a single estimate as they combine several estimates from different models. So the result may be a model with higher stability.

- **Bagging:**
  It is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average.
  
  - **Bootstrap Aggregating**, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression.
  - It decreases the variance and helps to avoid overfitting. It is usually applied to decision tree methods. Bagging is a special case of the model averaging approach.

- **Boosting:** 
  It is also a homogeneous weak learners’ model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm.

    - Boosting is designed to create a strong classifier by combining multiple weak classifiers.
    - The process involves building models sequentially, where each new model aims to correct the errors made by the previous ones.


##### Difference between bagging and boosting

| Bagging                                                                                                                                        | Boosting                                                                                                                                       |
| ---------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| The simplest way of combining predictions that belong to the same type.                                                                        | A way of combining predictions that belong to the different types.                                                                             |
| Aim to decrease variance, not bias.                                                                                                            | Aim to decrease bias, not variance.                                                                                                            |
| Each model receives equal weight.                                                                                                              | Models are weighted according to their performance.                                                                                            |
| Each model is built independently.                                                                                                             | New models are influenced by the performance of previously built models.                                                                       |
| Different training data subsets are selected using row sampling with replacement and random sampling methods from the entire training dataset. | Iteratively train models, with each new model focusing on correcting the errors (misclassifications or high residuals) of the previous models. |
| Bagging tries to solve the over-fitting problem.                                                                                               | Boosting tries to reduce bias.                                                                                                                 |
| If the classifier is unstable (high variance), then apply bagging.                                                                             | If the classifier is stable and simple (high bias) the apply boosting.                                                                         |
| In this base classifiers are trained parallelly.                                                                                               | In this base classifiers are trained sequentially.                                                                                             |
| Example: The Random forest model uses Bagging.                                                                                                 | Example: The AdaBoost uses Boosting techniques                                                                                                 |

## Biological Neuron


### Biological neuron and its functioning

The human brain is made up of 86 billion nerve cells. These nerve cells are called neurons.

> [!info] Neurons are the fundamental units of a neural network and are often referred to as information messengers, as they send and receive signals from other neurons. 

  The typical nerve cell of the human brain comprises of four parts:

1) **Dendrites**:
- A single neuron may have more than one set of dendrites and may receive many thousands of input signals.
- **Major Role**: Signals are received by the cell nucleus through dendrites and are processed and understood in the cell nucleus. Incoming signals can be either excitatory—which means they tend to make the neuron fire (generate an electrical impulse)—or inhibitory, which means that they tend to keep the neuron from firing.
- Whether or not a neuron is excited into firing an impulse depends on the sum of all the excitatory and inhibitory signals it receives.
- If the neuron does end up firing, the nerve impulse, or **action potential**, is conducted down the axon.

2) **Soma (Cell Body)**:
- Neurons, like other cells, have a cell body (called the soma).
- The nucleus of the neuron is found in the soma.
- **Major Role**: It sums all the incoming signals from dendrites to generate input.

3) **Axon Structure**:
- Axons differ from dendrites in several ways.
- The dendrites tend to taper and are often covered with little bumps called spines. In contrast, the axon tends to stay the same diameter for most of its length and doesn’t have spines.
- The axon arises from the cell body at a specialized area called the **axon hillock**.
- Finally, many axons are covered with a special insulating substance called **myelin**, which helps them convey the nerve impulse rapidly. Myelin is never found on dendrites.
- Towards its end, the axon splits up into many branches and develops bulbous swellings known as axon terminals (or nerve terminals). These axon terminals make connections on target cells.
- **Major Role**: It carries nerve impulses away from the cell body.

4) **Synapses**:
- It is the point of interconnection of one neuron with other neurons.
- **Major Role**: Synapses establish a connection to other neurons and complete the signal transformation.
- The amount of signal transmitted depends upon the strength (synaptic weights) of the connections.
- The connections can be inhibitory (decreasing strength) or excitatory (increasing strength) in nature.
- So, a neural network, in general, has a connected network of billions of neurons with a trillion of interconnections between them.

## ANN

### Basics of Artificial Neuron

- The human brain consists of about 86 billion neurons and more than 100 trillion synapses.
- In artificial neural networks, the number of neurons is about 10 to 1000.
- Artificial neurons (also called **Perceptrons**, **Units**, or **Nodes**) are the simplest elements or building blocks in a neural network.
- They are inspired by biological neurons that are found in the human brain.

### Components of the basic Artificial Neuron

Each artificial neuron comprises four different parts:

1. **Input Values**: These are the set of values for which we need to predict an output value. They can be viewed as features or attributes in a dataset.
2. **Weights**: Weights are the real values that are attached with each input/feature and they convey the importance of that corresponding feature in predicting the final output. In simple terms, the real value of a feature is called Weights. Weights are the values that determine the strength between two neurons.
3. **Bias**: The bias's purpose is to change the value that the activation function operates.
4. **Summation Function**: The work of the summation function is to bind the weights and inputs together and calculate their sum.
5. **Activation Function**: Each neuron has an activation function that is used to introduce non-linearity in the model. To standardise the output form the neurons the activation function is used. Activation functions are the mathematical equations that calculate the output of the neural network.

### Characteristics of ANN

- Artificial Neural Network is analogous to a biological neural network.
- A biological neural network is a structure of billions of interconnected neurons in a human brain. The human brain comprises of neurons that send information to various parts of the body in response to an action performed.
- Similar to this, an Artificial Neural Network (ANN) is a computational network in science that resembles the characteristics of a human brain.
- ANN can be modelled as the original neurons of the human brain, hence ANN processing parts are called **Artificial Neurons**.
- These are computational models and are inspired by the human brain. They are the biologically inspired simulations performed on the computer to perform certain specific tasks like - **Clustering, Classification, Pattern Recognition**.
- In general It is a biologically inspired network of artificial neurons configured to perform specific tasks.
- As the name implies, an Artificial Neuron Network (ANN) is a network of artificial neurons. It's a non-linear mapping structure, which is based on how biological nerve systems, such as the brain, process information.
- ANN learns from the training data (input and target output known) without any programming.
- The learned neural network is called an expert system with the capability to analyse information and answer the questions of a specific field.
- The formal definition of ANN given by Dr.Robert Hecht-Nielson, inventor of one first neuro computers is:

> [!quote] ... a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs 

### Characteristics Of ANN

- **Non Linearity:** The mechanism followed in ANN for the generation of the input signal is nonlinear.
- **Supervised Learning:** The input and output are mapped and the ANN is trained with the training dataset.
- **Unsupervised Learning:** The target output is not given, so the ANN will learn on its own by discovering the features in the input patterns.
- **Adaptive Nature:** The connection weights in the nodes of ANN are capable to adjust themselves to give the desired output.
- **Biological Neuron Analogy:** The ANN has a human brain-inspired structure and functionality.
- **Fault Tolerance:** These networks are highly tolerant as the information is distributed in layers and computation occurs in real-time.


### Layers of Artificial neural networks

- Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer.
- Each node, or artificial neuron, connects to another and has an associated weight and threshold.
- If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.

#### Input Layer

- The first or primary layer of the ANN is the input layer.
- In a biological neural network, the brain receives the input in the form of sensations received from the receptors. Similar to this, the input layer receives the input data in the form of image pixels, texts, audio files, and binary data.
- The Input layers contain those artificial neurons (termed as units) which are to receive input from the outside world.
- This is where the actual learning on the network happens, or recognition happens else it will process.
- The neurons in this layer receive external input signals and perform no computation but simply transfer the input signal to the neurons in other layers.
#### Hidden Layer

- After receiving the signal from the receptors, the information is sent to the brain for processing. Similarly, in ANN, the received information then traverses to the Hidden Layer for processing.
- The network may consist of a single input layer as in the case of perceptron or a series of multiple hidden layers.
- The hidden layers as mentioned are hidden in between input layers and the output layers.
- The only job of a hidden layer is to transform the input into something meaningful that the output layer/unit can use in some way.
- It performs various mathematical computations on the raw data. The results of these layers are the patterns that the network identifies after processing them.
#### Output Layer

- After the rigorous processing of the data in the hidden layers, the information is sent to the output layer.
- The various parameters that we feed to the model, affect the performance of the model. Hence, the output of the ANN model is generally dependent on the input parameters.
- Examples of these parameters may include weights, biases, learning rate and others.


## Activation Functions



## Types of Artificial Neural Networks


### FeedForward ANN

- FeedForward ANNS deals with the unidirectional flow of the data. That means, in feedforward ANN, the data flow from the input layer to the hidden layer and then finally to the output layer without any branching or backtracking.
- Feedback loops are absent in this network.
- These networks are predominantly used for Supervised Machine Learning algorithms like Classification, Regression, Image Recognition and so on.
- This network is useful when we want to deal with data that is not sequential.
### FeedBack ANN

- As opposed to the case of FeedForward networks, Feedback networks have a feedback loop present in them.
- These networks are beneficial in cases where memory retention is a key factor.
- The prime usage of these can be seen in the form RNN. These are mainly used with problems that deal with the sequential data type.


# Unit 3

### Expectation-Maximization (EM) Algorithm*

> [!info] The Expectation-Maximization (EM) algorithm is an iterative optimization method that combines different unsupervised machine learning algorithms to find maximum likelihood or maximum posterior estimates of parameters in statistical models that involve unobserved latent variables. 

- The EM algorithm is commonly used for latent variable models and can handle missing data. It consists of an estimation step (E-step) and a maximization step (M-step), forming an iterative process to improve model fit.

- In the E step, the algorithm computes the latent variables i.e. expectation of the log-likelihood using the current parameter estimates. 

- In the M step, the algorithm determines the parameters that maximize the expected log-likelihood obtained in the E step, and corresponding model parameters are updated based on the estimated latent variables. 

#### Expectation-Maximization in EM

By iteratively repeating these steps, the EM algorithm seeks to maximize the likelihood of the observed data. It is commonly used for unsupervised learning tasks, such as clustering, where latent variables are inferred and has applications in various fields, including machine learning, computer vision, and natural language processing.

***Key Terms in Expectation-Maximization (EM) Algorithm***

Some of the most commonly used key terms in the Expectation-Maximization (EM) algorithm are as follows:

1. **Latent Variables:** Latent variables are unobserved variables in statistical models that can only be inferred indirectly through their effects on observable variables. They cannot be directly measured but can be detected by their impact on the observable variables.
2. Likelihood: It is the probability of observing the given data given the parameters of the model. In the EM algorithm, the goal is to find the parameters that maximize the likelihood.
3. Log-Likelihood: It is the logarithm of the likelihood function, which measures the goodness of fit between the observed data and the model. EM algorithm seeks to maximize the log-likelihood.
4. Maximum Likelihood Estimation (MLE): MLE is a method to estimate the parameters of a statistical model by finding the parameter values that maximize the likelihood function, which measures how well the model explains the observed data.
5. Posterior Probability: In the context of Bayesian inference, the EM algorithm can be extended to estimate the maximum a posteriori (MAP) estimates, where the posterior probability of the parameters is calculated based on the prior distribution and the likelihood function.
6. Expectation (E) Step: The E-step of the EM algorithm computes the expected value or posterior probability of the latent variables given the observed data and current parameter estimates. It involves calculating the probabilities of each latent variable for each data point.
7. Maximization (M) Step: The M-step of the EM algorithm updates the parameter estimates by maximizing the expected log-likelihood obtained from the E-step. It involves finding the parameter values that optimize the likelihood function, typically through numerical optimization methods.
8. Convergence: Convergence refers to the condition when the EM algorithm has reached a stable solution. It is typically determined by checking if the change in the log-likelihood or the parameter estimates falls below a predefined threshold.

***How Expectation-Maximization (EM)  Algorithm Works:****

The essence of the Expectation-Maximization algorithm is to use the available observed data of the dataset to estimate the missing data and then use that data to update the values of the parameters. 

Initialization:
Initially, a set of initial values of the parameters are considered. A set of incomplete observed data is given to the system with the assumption that the observed data comes from a specific model.
E-Step (Expectation Step): In this step, we use the observed data in order to estimate or guess the values of the missing or incomplete data. It is basically used to update the variables. 
Compute the posterior probability or responsibility of each latent variable given the observed data and current parameter estimates.
Estimate the missing or incomplete data values using the current parameter estimates.
Compute the log-likelihood of the observed data based on the current parameter estimates and estimated missing data.
M-step (Maximization Step): In this step, we use the complete data generated in the preceding “Expectation” – step in order to update the values of the parameters. It is basically used to update the hypothesis.
Update the parameters of the model by maximizing the expected complete data log-likelihood obtained from the E-step.
This typically involves solving optimization problems to find the parameter values that maximize the log-likelihood.
The specific optimization technique used depends on the nature of the problem and the model being used.
Convergence: In this step, it is checked whether the values are converging or not, if yes, then stop otherwise repeat step-2 and step-3 i.e. “Expectation” – step and “Maximization” – step until the convergence occurs.
Check for convergence by comparing the change in log-likelihood or the parameter values between iterations.
If the change is below a predefined threshold, stop and consider the algorithm converged.
Otherwise, go back to the E-step and repeat the process until convergence is achieved



## Reinforcement learning


> [!info] In Reinforcement Learning (RL), agents are trained on a reward and punishment mechanism. The agent is rewarded for correct moves and punished for the wrong ones. In doing so, the agent tries to minimize wrong moves and maximize the right ones. 

 - It mimics the **trial-and-error** learning process that humans use to achieve their goals. Software actions that **work towards** your goal are **reinforced**, while actions that **detract** from the goal are **ignored**.

### Key Terms in Reinforcement Learning:

1. **Agent**:
   The learner or decision-maker that interacts with the environment to take actions. Example: A robot learning to navigate a maze.
2. **Environment**:
   The world or domain in which the agent operates and interacts. The environment provides feedback to the agent based on the actions taken. Example: The maze in which the robot is navigating.
3. **State (S)**:
   A representation of the current situation or configuration of the environment. Example: The current location of the robot in the maze.
4. **Action (A)**:
   A set of possible moves or decisions the agent can take in the given state. Example: Moving left, right, forward, or backward in the maze.
5. **Reward (R)**:
   A positive, negative, or zero value - in other words, the **reward or punishment** for taking an action
6. **Policy (π)**:
   A strategy or mapping from states to actions, representing the agent's behavior. It defines what action the agent will take in a given state. Example: A policy might tell the robot to always move towards the nearest open space in the maze.


### Difference between negative and positive reinforcement learning

| **Aspect**                | **Positive Reinforcement**                                     | **Negative Reinforcement**                                        |
|---------------------------|---------------------------------------------------------------|-------------------------------------------------------------------|
| **Definition**             | Rewards the agent for desired or beneficial actions.           | Removes a negative condition after the agent performs the desired action. |
| **Effect on Behavior**     | Increases the likelihood of repeating the rewarded action.     | Increases the likelihood of avoiding actions that lead to negative outcomes. |
| **Purpose**                | Encourages the agent to seek out actions that lead to rewards. | Encourages the agent to avoid or eliminate actions causing penalties or discomfort. |
| **Example**                | Rewarding the agent with points for reaching a goal.           | Preventing or reducing penalties (e.g., stopping a punishment) after the agent avoids an undesirable state. |
| **Learning Outcome**       | The agent learns which actions lead to positive rewards.       | The agent learns to avoid actions that lead to negative conditions. |
| **Common Use**             | Typically used to promote desired actions and behaviors.       | Typically used to discourage harmful or ineffective behaviors.     |

### Difference between active and passive reinforcement learning

| **Aspect**                 | **Active Reinforcement Learning**                               | **Passive Reinforcement Learning**                              |
|----------------------------|-----------------------------------------------------------------|----------------------------------------------------------------|
| **Action Selection**        | Agent **actively chooses** actions to maximize rewards.         | Agent **follows a fixed policy** and does not choose actions.   |
| **Goal**                    | To find the optimal policy by exploring the environment.        | To evaluate the given policy and learn the value of states.     |
| **Exploration**             | Agent explores and balances exploration and exploitation.       | No exploration; agent only observes outcomes of the fixed policy. |
| **Policy**                  | The agent learns and improves the policy over time.            | The policy is fixed and not improved by the agent.              |
| **Example**                 | Agent tries different paths to find the optimal route in a maze.| Agent follows a predefined path and learns the value of states. |
| **Focus**                   | Learning to improve action selection and maximize future rewards. | Evaluating the effectiveness of the current policy.             |

This table highlights the key differences between how active and passive reinforcement learning approaches handle action selection and learning.

### Applications of Reinforcement Laerning

[[#Reinforcement Learning]]


### Write in short about various reinforcement algorithm. 

#### Q-Learning
- **Type**: Model-free, Off-policy
- **Description**: Q-learning learns the value of actions in a given state (Q-values) without requiring a model of the environment. The agent updates its Q-values using the Bellman equation and chooses actions based on the learned values.
- **Use Case**: Simple environments, games, or scenarios where the environment is not modeled.

#### SARSA (State-Action-Reward-State-Action)
- **Type**: Model-free, On-policy
- **Description**: Similar to Q-learning, but updates Q-values using the action actually taken by the agent (on-policy). It learns the value of the current policy.
- **Use Case**: When the agent’s policy is continuously evolving and learning from its own actions.

#### Deep Q-Network (DQN)
- **Type**: Model-free, Off-policy, Deep Learning
- **Description**: Combines Q-learning with deep neural networks to handle large, complex state spaces. DQN approximates the Q-values using a neural network.
- **Use Case**: Complex problems like Atari games, where traditional Q-learning cannot handle the large state space.

#### Actor-Critic Methods
- **Type**: Hybrid (combines policy-based and value-based)
- **Description**: The actor decides what action to take, and the critic evaluates the action taken by estimating the value function. These methods combine the strengths of both value-based and policy-based approaches.
- **Use Case**: Used for faster convergence and in continuous action spaces.


### Explain the policy and types of policy

- In reinforcement learning, a **policy** is a strategy or a function that defines the agent’s behavior by determining which action to take at a given state.
- The policy is key to how the agent interacts with the environment and makes decisions to maximize cumulative rewards.

#### Types of Policy

1. **Deterministic Policy**:
    - **Definition**: A policy where the agent always selects the same action for a given state.
    - **Notation**: π(s)=a\pi(s) = aπ(s)=a, meaning the policy π\piπ maps a state sss to a specific action aaa.
    - **Use Case**: Useful when the environment is predictable and there is a clear optimal action for each state.
2. **Stochastic Policy**:
    - **Definition**: A policy where the agent selects actions probabilistically, meaning different actions can be taken in the same state based on a probability distribution.
    - **Notation**: π(a∣s)\pi(a|s)π(a∣s), representing the probability of taking action aaa in state sss.
    - **Use Case**: Suitable for environments with uncertainty or where exploration of different actions is required to learn optimal behavior.

# Practice Questions

## Practice theory question based on unit 1 

### Chapter 1

1. Define Intelligence. Explain types of Intelligence.
2. Define Artificial Intelligence. Explain types of Artificial Intelligence.
3. Explain various approaches of Artificial Intelligence. (Humanly and rationally)
4. Short note on foundation of AI. [[#Foundations |answer]]
5. Explain the history of AI. [[#History |answer]]
6. Short note on State of Art of AI. [[#State of the Art |answer]]
### Chapter 2

1. Define Agents. Explain in detail the architecture of Agents. Explain following terms: Perception,  Action, Sensors, Actuator. [[#Intelligent Agents]]
2. Explain the characteristics of Intelligent agents. [[#Characteristics of intelligent agents |answer]]
3. What do you understand by Rational Agent. [[#Rational Agents |answer]]
4. What is PEAS. Explain the PEAS for given system.
5. What is Environment in AI. Explain the various types of environment in AI.
6. Explain in detail Simple Reflex Agents.
7. Explain in detail Model based Agents 
8. Explain in detail Goal Based Agent [[#Goal Based Agents |answer]]
9. Explain in detail Utility Based Agents [[#Utility Based Agents |answer]]
10. Explain in detail Learning Agents [[#Learning Agents |answer]]

### Chapter 3

1) Short note on BFS. Explain the steps to find BFS. State it's advantages and disadvantages.
2) Short note on DFS. Explain the steps to find DFS. State it's advantages and disadvantages.
3) Short note on  DLS. Explain the steps to find DLS. State it's advantages and disadvantages.
4) Short note on IDDFS. Explain the steps to find IDDFS. State it's advantages and disadvantages.
5) Short note on Uniform cost search.
6) Explain Greedy first search algorithm in detail.
7) Explain $A^*$ algorithm in detail with steps. Explain $f(n),\ g(n),\ h(n)$. 
8) Numerical question based on BFS, DFS, IDDFS, DLS, UCS and $A^*$ algorithm.
9) Short note on Informed Search
10) Short note on Uninformed Search 
11) Explain the difference between informed and uninformed Search 
12) What is Heuristic function.